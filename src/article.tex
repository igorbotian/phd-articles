% sudo apt-get install texlive-latex-base
% sudo apt-get install texlive-fonts-extra
% sudo apt-get install texlive-latex-recommended
% sudo apt-get install texlive-lang-cyrillic

\documentclass[10pt,a5paper,oneside]{article}

\title{Минимаксная классификация интервальных оценок предпочтений с использованием машины опорных векторов}
\author{Л.В. Уткин}
\date{\today}

\usepackage[utf8]{inputenc} % возможность использования Unicode-символов в исходных файлах

\usepackage{amsmath} % для поддержки расширенных математических символов
\usepackage{amsfonts} % для поддержки математических шрифтов
\usepackage{amssymb} % для поддержки расширенных математических символов
\usepackage{color} % для поддержки цвета текста, отличного от чёрного
\usepackage{datetime} % поддержка календаря и дат
\usepackage{dashrule} % для вывода горизонтальной линии
\usepackage{fancyhdr} % использование собственного формата заголовков
\usepackage[T2A]{fontenc} % задание шрифта
\usepackage{geometry} % задания страничных отступов
\usepackage{graphicx} % поддержка PNG-формата в качестве иллюстраций
\usepackage[pdftex,bookmarks,unicode]{hyperref} % поддержка возможности использования ссылок в качестве гиперссылок
\usepackage{indentfirst} % отступ первого параграфа в разделе
\usepackage{setspace} % задание межстрочного интервала
\usepackage{verbatim} % скрытие "comment-окружений" из результирующего документа
\usepackage[english,russian]{babel} % поддержка переносов слов

\hypersetup{
%	bookmarkstype=none,
%	plainpages=false,
	pdftitle={Минимаксная классификация интервальных оценок предпочтений с использованием машины опорных векторов},
	pdfauthor={И.Ю. Ботян, Л.В. Уткин},
	pdfsubject={Статья},
	pdfnewwindow=false,
	pdfkeywords={машинное обучение, ранжирование, машина опорных векторов, интервальные предпочтения экспертов, теория Демпстера-Шефера},
	colorlinks=true,
	linkcolor=black,
	citecolor=black,
	filecolor=black,
	urlcolor=black,
	pageanchor=false} % если пакет "lastpage" импортирован, то необходимо отключить данную опцию

% задание страничных отступов
\geometry{left=2.0cm}
\geometry{right=2.0cm}
\geometry{top=2.0cm}
\geometry{bottom=2.5cm}

\onehalfspacing % задание полуторного межстрочного интервала
\setlength{\parindent}{7mm} % задание абзацного отступа в 7 миллиметров
\setlength{\headheight}{28pt}

\begin{document}

\input{tex_definitions}
\ApplyCommonPageStyle

%======================================================================================================================

\centerline{И.Ю.~Ботян\footnote{И.Ю.~Ботян - igor.botian@gmail.com; Санкт-Петербургский государственный лесотехнический университет, Санкт-Петербург}, Л.В.~Уткин\footnote{Л.В.~Уткин - д.т.н, проф., lev.utkin@mail.ru; Санкт-Петербургский государственный лесотехнический университет, Санкт-Петербург}}
\centerline{\textbf{МИНИМАКСНАЯ КЛАССИФИКАЦИЯ}}
\centerline{\textbf{ИНТЕРВАЛЬНЫХ ОЦЕНОК ПРЕДПОЧТЕНИЙ }}
\centerline{\textbf{С ИСПОЛЬЗОВАНИЕМ}}
\centerline{\textbf{МАШИНЫ ОПОРНЫХ ВЕКТОРОВ}}

\vspace*{1em}
\rule{9cm}{1pt}

\par
{\footnotesize %
\textbf{Аннотация.} В статье предлагается новый подход к классификации документов на основе интервальных экспертных оценок, который основывается на теории свидетельств Демпстера-Шефера и модификации машины опорных векторов. 
Разработана соответствующая задача квадратичной оптимизации. 
Получена её двойственная форма в терминах множителей Лагранжа, позволяющая использовать нелинейность для построения более адекватной модели. %
}

\par
{\footnotesize %
\textbf{Ключевые слова:} машинное обучение, ранжирование, машина опорных векторов, интервальные предпочтения экспертов, теория Демпстера-Шефера.%
}

\rule{9cm}{1pt}
\vspace*{1em}

%======================================================================================================================

\par
\Section{Введение}. 
Порядковая регрессия, являясь обобщением линейной модели регрессии, характерна тем, что зависимая переменная измеряется в порядковой шкале. 
Вместе с этим в ней рассматривается задача, которая имеет свойства, присущие классификации в целом, то есть класс $y$ является конечным множеством, элементы которого упорядочены \reference{Herbrich2000}.
Тогда класс \emph{0} может соответствовать понятию ``нерелевантности'', а класс \emph{y\sub{max}} - ``наибольшей релевантности''. 
Данные классы используются в задачах классификации, где каждый элемент заданного множества документов соотносится с тем или иным классом. 
А если между классами задан некий порядок, то имеет место задача ранжирования документов. 

\par
Как правило, классы определяются \emph{экспертами}.
А вместе со связанными с ними предпочтениями они формируют обучающую выборку, необходимую для решения задачи классификации. 

\par
Стоит отметить, что существующие на данный момент подходы к решению задачи классификации основываются на предположении о том, что экспертные оценки являются простыми, когда одному классу ставится в соответствие другой. 
Тогда как в реальности существуют ситуации, в которых проводится соответствие между двумя группами (множествами) классов. 

\par
В настоящий момент не существует обоснованных методов решения задачи ранжирования на основе групповых экспертных оценок. 
В статье предлагается новый подход к ранжированию документов, позволяющий работать с такими оценками. 
Он основан на применении машины опорных векторов с использованием теории Демпстера-Шефера. 
Разработана соответствующая задача квадратичного программирования конечной размерности. 

%======================================================================================================================

\vspace*{1em}

\par
\Section{Стандартная постановка задачи классификации}. 
Для начала необходимо рассмотреть стандартную постановку задачи бинарной классификации.
Пусть дано множество, называемое обучающей выборкой:
\mbox{}
\[
(\mathbf{x_1}, y_1),(\mathbf{x_2}, y_2), \dots, (\mathbf{x_n}, y_n) \in \mathbf{\chi} \times \{-1,+1\}
\]

\par
Здесь \(\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_n}\) является непустым множеством образцов наблюдений, принимающих значения из \(\mathcal{X}\) и имеющие $l$ признаков; \(y_1, \dots, y_n\) - классы, принимающие два значения - \(-1,+1\). 
Предполагается, что данные, приведённые выше, подчиняются неизвестной функции распределения \(F_0(\mathbf{x}, \emph{y})\) над множеством \(\mathbf{\chi} \times \{-1, +1\}\). 

\par
Целью классификации является поиск решающей функции \(g(\mathbf{x})\), прогнозирующей класс \emph{y} для нового образца \(\mathbf{x}\), который может не принадлежать обучающей выборке. 
Иными словами, необходимо найти функцию \emph{g}, которая минимизирует ошибку классификации. 

\par
Один из возможных подходов для решения данной задачи является поиск разделяющей функции \(f(\mathbf{x}, \mathbf{w})\), знак которой определяет класс: \(g(\mathbf{x})=sgn(f(\mathbf{x}, \mathbf{w}))\).
Её параметры \(\mathbf{w}=(w_0, w_1, \dots, w_n)\) определяются по обучающей выборке посредством алгоритма классификации. 
Она может быть как линейной, так и нелинейной.

\par
Параметры \(\mathbf{w}\) могут быть найдены путём минимизации функционала риска по множеству параметров функции \(f(\mathbf{x}, w)\):
\mbox{}
\[
R(\mathbf{w}) = \int \limits_{\chi \times \{-1, +1\}} L(f, \mathbf{x}, y) \mathrm{d} F_0(\mathbf{x}, y).
\]

Здесь функция потерь \(L(f, \mathbf{x}, y)\) обычно принимает ненулевое значение, когда прогнозируемый при помощи разделяющей функции класс не совпадает с реальным классом \emph{y}, т.е. имеет место ошибка классификации. 
Иначе эта ошибка равна нулю. 

\par
Эмпирический функционал риска определяется путём предположения, что функция \(F_0(\mathbf{x}, y)\) оценивается при помощи непараметрических методов \reference{Wolfowitz1942} и имеет скачки размером 1/\emph{n} в точках \(x_1, \dots, x_n\), т.е.
\mbox{}
\[
R_{emp}(\mathbf{w}) = n^{-1} \cdot \sum \limits_{k=1}^n L(f_k, \mathbf{x}_k, y_k).
\]

Здесь \(f_k=f(\mathbf{x_k}, w)\). 

%======================================================================================================================

\vspace*{1em}

\par
\Section{Задача классификации на основе попарного ранжирования}. 
В соответствии с попарным подходом, сравниваются пары \((\mathbf{x},\mathbf{z})\) из множества \emph{M} объектов. 
Здесь \(\mathbf{x}\) и \(\mathbf{z}\) - векторы с \emph{l} признаками, т.е. обучающая выборка состоит из пар объектов.

\par
В соответствии с идеей, предложенной Лиу \reference{Liu2011} и основывающейся на использовании математических ожиданий, каждая пара объектов \((\mathbf{x}, \mathbf{z})\) характеризуется классом \(y \in \{-1,1\}\), где \emph{y} = 1 означает, что данный объект с признаками \(\mathbf{x}\) является более предпочтительным, чем объект с признаками \(\mathbf{z}\), \(\mathbf{x} \succ \mathbf{z}\). 
Если \(y = -1\), то в таком случае \(\mathbf{z} \succ \mathbf{x}\). 
Тогда предполагается, что \((\mathbf{x}, \mathbf{z}, y)\) является случайной величиной с функцией распределения \(F_1\). 
Функционал риска в данном случае определяется как:

\[
R(\mathbf{w}) = \int \limits_{\chi \times \chi \times \{-1, 1\}} L(f, \mathbf{x}, \mathbf{z}, y) \mathrm{d} F_1(\mathbf{x}, \mathbf{z}, y)
\]

\par
Этот функционал риска означает ожидаемые потери, получающиеся в результате неправильной классификации пар объектов некоторой разделяющей функцией \emph{f}. 
Пусть имеется множество \emph{Q} из \emph{n} предпочтений, основывающихся на сравнительных оценках:
\mbox{}
\[
Q = \{(\mathbf{x_1}, \mathbf{z_1}, y_1), (\mathbf{x_2}, \mathbf{z_2}, y_2), \dots, (\mathbf{x_n}, \mathbf{z_n}, y_n)\}
\]

\par
Задача классификации может быть сформулирована как вычисление ранжирующей функции \emph{f} из некоторого параметрического множества \emph{F}, такого, что \(f(\mathbf{x}, \mathbf{w}) \geq f(\mathbf{z}, \mathbf{w})\) при \(\mathbf{x} \succ \mathbf{z}\). 
Если предположить, что обучающая выборка не может одновременно содержать два элемента \((\mathbf{x}, \mathbf{z}, 1)\) и \((\mathbf{z}, \mathbf{x}, -1)\), то величина \emph{y} для краткости может быть опущена. 
Обычный способ решения этой задачи заключается в рассмотрении эмпирического функционала риска:
\mbox{}
\[
R_{emp}(\mathbf{w}) = \frac{1}{n} \sum \limits_{i=1}^n L(f_i, \mathbf{x_i}, \mathbf{z_i}).
\]

Тогда задача классификации может быть сформулирована как задача поиска ранжирующей функции \(f(\mathbf{x}, \mathbf{w})\), минимизирующей эмпирический функционал риска. 
Для её поиска можно воспользоваться одним из наиболее известных подходов - применении модели RankSVM, строящейся путём минимизации целевой функции \reference{Herbrich2000}:
\mbox{}
\[
\frac{1}{2} <w, w> + C \cdot \sum \limits_{i=1}^n L(f_i, \mathbf{x_i}, \mathbf{z_i}).
\] 

При этом функция потерь в RankSVM является петлевой, определённой на паре объектов \(\mathbf{x} \succ \mathbf{z}\), то есть она имеет следующий вид:
\mbox{}
\[
L(f, \mathbf{x}, \mathbf{z}, y) = max(0, 1 - (f(\mathbf{x}, \mathbf{w}) - f(\mathbf{z}, \mathbf{w}))).
\]

Если ввести переменные оптимизации \(\xi_i = max(0, 1 - (f(\mathbf{x_i}, \mathbf{w}) - f(\mathbf{z_i}, \mathbf{w}))\), то задача оптимизации может быть переписана как:
\mbox{}
\[
\frac{1}{2}<w, w> + C \cdot \sum \limits_{i=1}^n \xi_i,
\]

при ограничениях
\mbox{}
\[
f(\mathbf{x_i}, w) - f(\mathbf{z_i}, w) \geq 1 - \xi_i, 
\xi_i \geq 0, i = 1, \cdots, n.
\]

Если ранжирующая функция линейна, то исходная задача сводится к задаче квадратичного программирования.

%======================================================================================================================

\vspace*{1em}

\par
\Section{Основные элементы теории Демпстера-Шефера}. 
Пусть \emph{U} - универсальное множество (\emph{фрейм различения}). 
Предположим, что было получено \emph{N} наблюдений элемента множества \(u \in U\).
При этом каждое наблюдение получено в результате неточного измерения, приводящего к множеству значений \emph{A}.
Пусть \(c_i\) как количество появлений \(A_i \subseteq U\), а \(\Rho_o(U)\) - множество всех подмножеств \emph{U}.
Частотная функция \emph{m}, называемая \emph{базовой вероятностью (БВ)}, определена как \reference{Dempster1967}:

\[
m : \Rho_o(U) \to [0,1], m(\varnothing) = 1, \sum \limits_{A \in \Rho_o(U)} m(A) = 1.
\]

\par
В соответствии с \reference{Dempster1967}, эта функция может быть получена как \(m(A_i) = c_i / N.\). 
Если существует функция \(h(x)\), то её верхнее математическое ожидание может быть найдено как \reference{Strat1990}:

\[
\mathbb{\overline{E}} h = \sum \limits_{i=1}^N m(A_i) \inf_{x \in A_i} h(x).
\]

\par
Нижнее математическое ожидание может быть найдено аналогичным образом. 

%======================================================================================================================

\vspace*{1em}

\par
\Section{Формальная постановка задачи классифкации в терминах неточных сравнений}. 
Предположим, что есть \emph{M} объектов для сравнения \(\mathbf{\Psi} = \{\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_M}\}\). 
Отсюда следует, что имеется \(N = M(M - 1)\) пар для сравнения. 
Эти \emph{N} пар \((\mathbf{x}, \mathbf{z})\) составляют фрейм различения \(\Omega\). 
Предположим, что обучающая выборка состоит из \(b \leq N\) оценок вида \(\mathbf{A} \succ \mathbf{B}\), где \(\mathbf{A}\) и \(\mathbf{B}\) - некоторые дискретные множества объектов, таких, что \(\mathbf{A} \cap \mathbf{B} = \varnothing\).  
Точные оценки \(\mathbf{x} \succ \mathbf{z}\) могут рассматриваться как частный случай. 
``Неточные'' оценки означают отсутствие информации о том, как элементы из \(\mathbf{A}\) сравнимы между собой. 

\par
Базовые вероятности неточных оценок могут быть определены следующим образом. 
Пусть \emph{n} - множество пар индексов \((i, j)  \in \Rho \), соответствующих оценкам \(\mathbf{A_i} \succ \mathbf{B_j}\). 
Оценка базовой вероятности \(\mathbf{A_i} \succ \mathbf{B_j}\) определяется как \(m(\mathbf{A_i} \succ \mathbf{B_j}) = c_{ij} / n\), где \(c_{ij}\) - это количество оценок \(\mathbf{A_i} \succ \mathbf{B_j}\), пары индексов которых принадлежат множеству \(\Rho\). 
Далее для краткости предполагается, что \(c_{ij} = 1\) и \(m(\mathbf{A_i} \succ \mathbf{B_j}) = 1/n\) для всех наблюдаемых сравнений. 
Тогда можно записать верхнюю границу функционала риска как:
\mbox{}
\[
\overline{R}(\mathbf{w}) = \mathbb{\overline{E}}L = \frac{1}{n} \sum \limits_{(i,j) \in \Rho} \underset{%
\mathbf{x} \in \mathbf{A_i}, \mathbf{z} \in \mathbf{B_j}%
}{\operatorname{max}}%
L(f(\mathbf{x}, \mathbf{w}) - f(\mathbf{z}, \mathbf{w})).
\]

\par
В свою очередь, нижняя граница может быть получена путём замены максимума на минимум в формуле, приведённой выше. 

\par
Неточные оценки сравнений образует множество распределений вероятностей, где каждое распределение из множества может быть использовано для вычисления функционала риска.
Среди них можно выделить то, которое максимизирует оценку риска для каждого \(\mathbf{w}\). 
В этом случае имеет место минимаксная (пессимистичная) стратегия: выбирается ``наихудшее'' распределение, дающее наибольшее значение функционала риска. 
Этот критерий выступает как страховка против наихудшего случая, так как он нацелен на минимизацию потерь в наименее благоприятном случае \reference{Robert1994}. 
``Наилучшее'' распределение, обеспечивающее наименьшее значение функционала риска, приводит к миниминной (оптимистичной) стратегии.

%======================================================================================================================

\vspace*{1em}

\par
\Subsection{Минимаксная стратегия}. 
Наша цель состоит в минимизации верхней оценки риска над множеством параметров \(\mathbf{w}\). 
Введём новые переменные оптимизации \(G_{ij}, (i, j) \in \Rho\) такие, что

\[
G_{ij} = \underset{\mathbf{x} \in \mathbf{A_i}, \mathbf{z} \in \mathbf{B_j}}{\operatorname{max}} L (f (\mathbf{x}, \mathbf{w}) - f(\mathbf{z}, \mathbf{w})). 
\]

\par
Тогда задача оптимизации может быть переписана как \(\overline{R}(\mathbf{w}) = \underset{\mathbf{w}, G_{ij}}{\operatorname{min}} \sum \limits_{(i, j) \in \Rho} G_{ij},\) при ограничениях

\[
G_{ij} \geq L (f(\mathbf{x}, \mathbf{w}) - f(\mathbf{z}, \mathbf{w})), \forall \mathbf{x} \in \mathbf{A_i}, \forall \mathbf{z} \in \mathbf{B_j}, (i, j) \in \Rho
\]

\par
Здесь \(\mathbf{x}\) и \(\mathbf{z}\) могут быть взяты только из \(\Psi\). 
Дальше можно снова использовать линейную функцию \emph{f} и петлевую функцию потерь, определённую на паре объектов \(\mathbf{x} \succ \mathbf{z}\):

\[
L(f, \mathbf{x}, \mathbf{z}) = max (0,1 - f(\mathbf{x}, \mathbf{w}) + f(\mathbf{z}, \mathbf{w})),
\]

\par
В этом случае получем \(\overline{R}(\mathbf{w}) = \underset{\mathbf{w}, G_{ij}}{\operatorname{min}} \sum \limits_{(i, j) \in \Rho} G_{ij},
\) при ограничениях \(G_{ij} + \langle w, \mathbf{x} - \mathbf{z} \rangle \geq 1, \forall \mathbf{x} \in \mathbf{A_i}, \forall \mathbf{z} \in \mathbf{B_j}, G_{ij} \succ 0, (i, j) \in \Rho\).

\par
В результате получается задача линейного программирования. 
При этом она отличается от подобных задач наличием дополнительных ограничений для каждой переменной \(G_{ij}\). 

%======================================================================================================================

\vspace*{1em}

\par
\Section{Формулировка машины опорных векторов при неточных сравнениях}. 
Для ограничения класса возможных решений задачи классификации и предотвращения переобучения к целевой функции добавляется регуляризационное слагаемое \(\Omega[f] = \frac{1}{2}\|w\|^2 = \frac{1}{2} \langle w, w \rangle \). 
Предложенное Тихоновым, оно является наиболее популярным штрафом, который может рассматриваться как ограничение, обеспечивающее уникальность и эффективно ограничивающее пространство допустимых решений \reference{Tikhonov1977}. Тогда целевую функцию можно переписать следующим образом:

\[
\overline{R}(\mathbf{w}) = \frac{1}{2} \langle w, w \rangle + C \sum \limits_{(i, j) \in \Rho} G_{ij}. 
\]

\par
Здесь \emph{C} - это постоянный параметр ``стоимости'', устанавливающий компромисс между минимизацией функционала риска и гладкостью разделяющей функции \reference{Yu2009}. 
Исходная задача сводится к задаче квадратичной оптимизации, целевая функция которой будет выглядеть следующим образом, когда \(\mathbf{A_i} = \{\mathbf{x_i}\}\) и \(\mathbf{B_i} = \{\mathbf{z_i}\}\): 
\mbox{}
\begin{eqnarray*}
&L = \sum \limits_{(i, j) \in \Rho} \sum \limits_{\mathbf{x} \in \mathbf{A_i}, \mathbf{z} \in \mathbf{B_j}} \mu_{ij} (\mathbf{x}, \mathbf{z}) - \\
&- \sum \limits_{(i, j) \in \Rho} \sum \limits_{(r, t) \in \Rho} \sum \limits_{\mathbf{x_1} \in \mathbf{A_i}, \mathbf{z_1} \in \mathbf{B_j}} \sum \limits_{\mathbf{x_2} \in \mathbf{A_r}, \mathbf{z_2} \in \mathbf{B_t}} \\
&\mu_{ij} (\mathbf{x_1}, \mathbf{z_1}) \cdot \mu_{rt} (\mathbf{x_2}, \mathbf{z_2}) \cdot \langle \mathbf{x_1} - \mathbf{z_1}, \mathbf{x_2} - \mathbf{z_2} \rangle
\end{eqnarray*}

\par
при ограничениях
\mbox{}
\[
0 \leq \sum \limits_{\mathbf{x} \in \mathbf{A_i}, \mathbf{z} \in \mathbf{B_j}} \mu_{ij} (\mathbf{x}, \mathbf{z}) \leq C, (i, j) \in \Rho.
\]

\par
Полученные результаты могут быть применены в пространстве признаков большей размерности, \(\varphi(\mathbf{x})\), для некоторой нелинейной функции \(\varphi\). 
Это получается путём замены скалярных произведений между преобразованными векторами признаков \((\varphi(\mathbf{x_1}) - \varphi(\mathbf{z_1})) (\varphi(\mathbf{x_2}) - \varphi(\mathbf{z_2}))\) при помощи ядра Мерсера \(K(\mathbf{x_1}, \mathbf{z_1}, \mathbf{x_2}, \mathbf{z_2})\), где \reference{Herbrich2000}:
\mbox{}
\[
K(\mathbf{x_1}, \mathbf{z_1}, \mathbf{x_2}, \mathbf{z_2}) = K(\mathbf{x_1}, \mathbf{x_2}) - K(\mathbf{x_1}, \mathbf{z_2}) - K(\mathbf{z_1}, \mathbf{x_2}) + K(\mathbf{z_1}, \mathbf{z_2}).
\]

\par
В итоге задача оптимизации сводится к следующему виду:
\mbox{}
\begin{eqnarray*}
&L = \sum \limits_{(i, j) \in \Rho} \sum \limits_{\mathbf{x} \in \mathbf{A_i}, \mathbf{z} \in \mathbf{B_j}} \mu_{ij} (\mathbf{x}, \mathbf{z}) - \\
&- \sum \limits_{(i, j) \in \Rho} \sum \limits_{(r, t) \in \Rho} \sum \limits_{\mathbf{x_1} \in \mathbf{A_i}, \mathbf{z_1} \in \mathbf{B_j}} \sum \limits_{\mathbf{x_2} \in \mathbf{A_r}, \mathbf{z_2} \in \mathbf{B_t}} \\
&\mu_{ij} (\mathbf{x_1}, \mathbf{z_1}) \cdot \mu_{rt} (\mathbf{x_2}, \mathbf{z_2}) \cdot K(\mathbf{x_1}, \mathbf{z_1}, \mathbf{x_2}, \mathbf{z_2}).
\end{eqnarray*}

%======================================================================================================================

\par
\Section{Заключение}. 
В статье предложен новый подход к классификации документов при групповых экспертных оценках, который характеризуется использованием машины опорных векторов в качестве средства обучения и пессимистической стратегией принятия решений. 
Для анализа интервальных экспертных оценок предложен аппарат теории Демпстера-Шефера. 
В результате применения данного подхода разработана задача оптимизации и получена её двойственная форма в терминах множителей Лагранжа, позволяющая использовать нелинейность для построения более адекватной модели. 
Новый подход был реализован для попарной классификации, но он так же может быть реализован и для поточечной и посписочной классификации в рамках дальнейшей работы.  

%======================================================================================================================

\begin{thebibliography}{9}
	%1
	\bibitem{Herbrich2000} R. Herbrich, T. Graepel, K. Obermayer. Large margin rank boundaries for ordinal regression. Advances in Large Margin Classifiers, pp. 115–132. MIT Press, Cambridge, MA, 2000%
	\bibitem{Wolfowitz1942} J. Wolfowitz. Additive partition functions and a class of statistical hypotheses. Annals of Mathematical Statistics, 13(3), pp. 247–279, 1942%
	\bibitem{Liu2011} T.Y. Liu. Learning to Rank for Information Retrieval, Springer, 2011%
	\bibitem{Dempster1967} A.P. Dempster. Upper and lower probabilities induced by a multi-valued mapping. Annales of Mathematical Statistics, 38(2), pp. 325–339, 1967%
	\bibitem{Strat1990} T.M. Strat. Decision analysis using belief functions. International Journal of Approximate Reasoning, 4(5), pp. 391–418, 1990%
	\bibitem{Robert1994} C.P. Robert. The Bayesian Choice. Springer, New York, 1994%
	\bibitem{Tikhonov1977} A.N. Tikhonov, V.Y. Arsenin. Solution of Ill-Posed Problems. W.H. Winston, Washington DC, 1977%
	\bibitem{Yu2009} H. Yu, Y. Kim, S. Hwang. Rv-svm: An efficient method for learning ranking SVM. Advances in Knowledge Discovery and Data Mining, vol. 5476 of Lecture Notes in Computer Science, pp. 426–438. Springer Berlin / Heidelberg, 2009%
\end{thebibliography}

%======================================================================================================================

\vspace*{2em}

\centerline{I.Yu.~Botian, L.V.~Utkin}
\centerline{\textbf{SVM-based Interval Expert Jugdments Minimax Classification}}

\rule{9cm}{1pt}

\par
{\footnotesize %
\textbf{Abstract.} The paper deals with a new approach to classify documents on the basis of interval expert judgments. 
The approach is based on the Dempster-Shafer theory and a support vector machine modification. 
A corresponding optmization problem was formulated. 
Its dual form in terms of the Lagrange multipliers was given. 
The form allows to utilize non-linearity to build a more adequate model. 
%
}

\par
{\footnotesize %
\textbf{Keywords:} machine learning, ranking, support vector machine, interval experts judgments, Dempster-Shafer theory.%
}
}

\rule{9cm}{1pt}
\vspace*{1em}

\noindent
Igor Yu. Botian - M. Sci., igor.botian@gmail.com; Saint Petersburg Forest Technical University, Saint Petersburg \\
Lev V. Utkin - PhD, Dr. Sci., Associate Prof., lev.utkin@mail.ru; Saint Petersburg Forest Technical University, Saint Petersburg

\end{document}
