% sudo apt-get install texlive-latex-base
% sudo apt-get install texlive-fonts-extra
% sudo apt-get install texlive-latex-recommended
% sudo apt-get install texlive-lang-cyrillic

\documentclass[12pt,a4paper,oneside]{article}

\title{Общий подход к классификации и ранжированию документов при неточных сравнительных оценках}
\author{И.Ю. Ботян, Л.В. Уткин}
\date{\today}

\usepackage[utf8]{inputenc} % возможность использования Unicode-символов в исходных файлах

\usepackage{amsmath} % для поддержки расширенных математических символов
\usepackage{amsfonts} % для поддержки математических шрифтов
\usepackage{amssymb} % для поддержки расширенных математических символов
\usepackage{color} % для поддержки цвета текста, отличного от чёрного
\usepackage{datetime} % поддержка календаря и дат
\usepackage{fancyhdr} % использование собственного формата заголовков
\usepackage[T2A]{fontenc} % задание шрифта
\usepackage{geometry} % задания страничных отступов
\usepackage{graphicx} % поддержка PNG-формата в качестве иллюстраций
\usepackage[pdftex,bookmarks,unicode]{hyperref} % поддержка возможности использования ссылок в качестве гиперссылок
\usepackage{indentfirst} % отступ первого параграфа в разделе
\usepackage{multicol} % разбивка текста на 2 колонки
\usepackage{parskip}
\usepackage{setspace} % задание межстрочного интервала
\usepackage{verbatim} % скрытие "comment-окружений" из результирующего документа
\usepackage[english,russian]{babel} % поддержка переноса слов

\hypersetup{
	pdftitle={Общий подход к классификации и ранжированию документов при неточных сравнительных оценках},
	pdfauthor={И.Ю. Ботян, Л.В. Уткин},
	pdfsubject={Статья},
	pdfnewwindow=false,
	pdfkeywords={машина опорных векторов, ранжирование документов, задача классификации},
	colorlinks=true,
	linkcolor=black,
	citecolor=black,
	filecolor=black,
	urlcolor=black,
	pageanchor=false}

% задание страничных отступов (в соответствии с требованиями)
\geometry{left=2.0cm}
\geometry{right=2.0cm}
\geometry{top=3.5cm}
\geometry{bottom=2.2cm}

\singlespacing % задание одиночного межстрочного интервала
\setlength{\parindent}{7mm} % задание абзацного отступа
\setlength{\parskip}{0pt}%
\setlength{\headheight}{15pt}

\begin{document}

\input{tex_definitions}
\ApplyCommonPageStyle{SCM-2014 Saint-Petersburg, 21-23 May 2014}

%======================================================================================================================

\begin{center}
\textbf{ОБЩИЙ ПОДХОД К КЛАССИФИКАЦИИ И РАНЖИРОВАНИЮ ДОКУМЕНТОВ ПРИ НЕТОЧНЫХ СРАВНИТЕЛЬНЫХ ОЦЕНКАХ}\\
\vspace*{1em}
И.Ю. Ботян, Л.В. Уткин\\
\vspace*{1em}
Санкт-Петербургский государственный лесотехнический университет\\
\vspace*{1em}
\end{center}

%======================================================================================================================

\begin{footnotesize}
\chapter{Abstract} \nolinebreak
The paper deals with an approach which allows to classify documents under group expert assessments. The approach is based on the Dempster-Shafer theory, utilizes the support vector machine and is characterized by usage of a pessimistic decision strategy. An appopriate optimization problem which allows to conudct classification under such the conditions is stated. 
\end{footnotesize}

%======================================================================================================================

\begin{multicols}{2}
\chapter{Введение}

\par
В порядковой регрессии рассматривается задача, которая имеет свойства, присущие как классификации в целом (i), так и метрической регрессии (ii). 
Как в (i) класс $y$ является конечным множеством, так и в (ii) присутствует упорядочение среди элементов $y$ \reference{Herbrich2000}. 
Например, класс \emph{0} может соответствовать понятию ``нерелевантности'', а класс \emph{y\sub{max}} -- понятию ``наибольшей релевантности''. 
Данные классы определяются как так называемыми \emph{экспертами}, так и, например, в случае веб-документов задаются посредством анализа переходов по ссылкам между ними (\emph{clickthrough data}). 

\par
В случае задания порядка между данными классами имеет место задача ранжирования. 
Наиболее близким к концепции ранжирования является попарный подход, направленный не на точное предсказывание степени релевантности каждого документа, а на определение относительного порядка между парой документов. 
При использовании данного подхода ранжирование обычно сводится к классификации пар документов, то есть к определению, какой из документов в паре является более предпочтительным.
Цель обучения в этом случае заключается в минимизации числа неправильно классифицированных пар документов. 

\par
В настоящее время существует большое количество моделей обучения ранжированию, основанных на попарном подходе, таких как машина опорных векторов, нейронные сети и другие. 
С процессом ранжирования связана проблема, имеющая место, когда пары документов не являются статистически независимыми, что приводит к нарушению основного предположения, лежащего в алгоритмах классификации. 
Хотя это не мешает использованию средств классификации для обучения модели ранжирования, необходима иная база необходима для анализа процесса обучения. 

\par
В статье предлагается новый подход к классификации документов на основе групповых экспертных оценок, который заключается в применении машины опорных векторов с использованием теории свидетельств Демпстера-Шефера и характеризуется пессимистической стратегией принятия решений. 
Основной целью данной статьи является постановка соответствующей задачи квадратичного программирования конечной размерности, которая позволяет проводить классификацию в условиях, указанных выше, и получается в результате применения данного подхода.

%======================================================================================================================

\vspace*{1em}
\chapter{Стандартная постановка задачи классификации}

\par
Для начала необходимо рассмотреть стандартную постановку задачи бинарной классификации.
Пусть дано множество, называемое обучающей выборкой:
\[
(\mathbf{x_1}, y_1),(\mathbf{x_2}, y_2), \dots, (\mathbf{x_n}, y_n) \in \mathbf{\chi} \times \{-1,+1\}
\]

\par
Здесь \(\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_n}\) является непустым множеством образцов наблюдений, принимающих значения из множества \(\mathcal{X}\) и имеющие $l$ признаков; \(y_1, \dots, y_n\) - классы, которые могут принимать два значения -- \emph{-1,+1}. 
Предполагается, что данные, приведённые выше, подчиняются неизвестной функции распределения \(F_0(\mathbf{x}, \emph{y})\) над множеством \(\mathbf{\chi} \times \{-1, +1\}\). 

\par
Целью классификации является поиск решающей функции \(g(\mathbf{x})\), прогнозирующей класс \emph{y} для нового образца \(\mathbf{x}\), который может не принадлежать обучающей выборке. 
Иными словами, необходимо найти функцию \emph{g}, которая минимизирует ошибку классификации. 

\par
Один из подходов для решения данной задачи является поиск разделяющей функции \(f(\bolsymbol{x}, \mathbf{w})\), знак которой определяет класс: \(g(\mathbf{x})=sgn(f(\mathbf{x}, \mathbf{w}))\).
Она имеет параметры \(\mathbf{w}=(w_0, w_1, \dots, w_n), \mathbf{w} \in \Lambda\), которые определяются по обучающей выборки посредством алгоритма классификации. 
Разделяющая функция может быть как линейной, так и нелинейной.

\par
Параметры \(\mathbf{w}\) могут быть найдены минимизацией функционала риска по множеству параметров функции \(f(\mathbf{x}, w), w \in \Lambda\):
\[
R(\mathbf{w}) = \int \limits_{\chi \times \{-1, +1\}} L(f, \mathbf{x}, y) \mathrm{d} F_0(\mathbf{x}, y).
\]

\par
Здесь функция потерь \(L(f, \mathbf{x}, y)\) обычно принимает ненулевое значение, когда прогнозируемый при помощи разделяющей функции класс не совпадает с реальным классом \emph{y}, то есть имеет место ошибка классификации. 
В остальных случаях эта ошибка равна нулю. 

\par
Эмпирический функционал риска определяется путём предположения, что функция \(F_0(\mathbf{x}, y)\) оценивается в рамках аппарата непараметрических методов \reference{Wolfowitz1942} и имеет скачки размером 1/\emph{n} в точках \(x_1, \dots, x_n\). То есть
\[
R_{emp}(\mathbf{w}) = n^{-1} \cdot \sum \limits_{k=1}^n L(f_k, \mathbf{x}_k, y_k).
\]

Здесь \(f_k=f(\mathbf{x_k}, w)\). 

%======================================================================================================================

\vspace*{1em}
\chapter{Задача классификации на основе попарного обучения ранжированию}

\par
В соответствии с попарным подходом, сравниваются различные пары \((\mathbf{x},\mathbf{z})\) из множества \emph{M} объектов. 
Здесь \(\mathbf{x}\) и \(\mathbf{z}\) - векторы с \emph{l} вещественнозначными признаками и принимают значения из множества \(\mathbf{\chi}\). 
Другими словами, обучающая выборка в попарном подходе состоит из пар объектов. 

\par
В соответствии с идеей, предложенной Лиу \reference{Liu2011} и основывающейся на использовании математических ожиданий, каждая пара объектов \((\mathbf{x}, \mathbf{z})\) характеризуется классом \(y \in \{-1,1\}\), где \emph{y} = 1 означает, что данный объект с признаками \(\mathbf{x}\) является более предпочтительным, чем объект с признаками \(\mathbf{z}\), \(\mathbf{x} \succ \mathbf{z}\). Если \(y = -1\), то в таком случае \(\mathbf{z} \succ \mathbf{x}\). Тогда предполагается, что \((\mathbf{x}, \mathbf{z}, y)\) является случайной величиной с функцией распределения \(F_1\). Функционал риска в данном случае определяется как:
\[
R(\mathbf{w}) = \int \limits_{\chi \times \chi \times \{-1, 1\}} L(f, \mathbf{x}, \mathbf{z}, y) \mathrm{d} F_1(\mathbf{x}, \mathbf{z}, y)
\]

\par

Этот функционал означает ожидаемые потери, получающиеся в результате неправильной классификации пар объектов некоторой разделяющей функцией \emph{f}. 

\par
Пусть имеется какое-то множество \emph{Q} из \emph{n} предпочтений, основывающихся на сравнительных оценках:
\[
Q = \{(\mathbf{x_1}, \mathbf{z_1}, y_1), \mathbf{x_2}, \mathbf{z_2}, y_2), \dots, \mathbf{x_n}, \mathbf{z_n}, y_n)\}
\]

\par
Задача классификации может быть сформулирована как вычисление ранжирующей функции \emph{f} из какого-то параметрического множества \emph{F}, такого, что \(f(\mathbf{x}, \mathbf{w}) \geq f(\mathbf{z}, \mathbf{w})\) при \(\mathbf{x} \succ \mathbf{z}\). 
Если предположить, что обучающая выборка не может одновременно содержать два элемента \((\mathbf{x}, \mathbf{z}, 1)\) и \((\mathbf{z}, \mathbf{x}, -1)\), то величина \emph{y} для краткости может быть опущена. 

\par
Обычный способ решения такой задачи классификации заключается в рассмотрении эмпирического функционала риска:
\[
R_{emp}(\mathbf{w}) = \frac{1}{n} \sum \limits_{i=1}^n L(f_i, \mathbf{x_i}, \mathbf{z_i}).
\]

\par
Тогда задача классификации может быть сформулирована как задача поиска ранжирующей функции \(f(\mathbf{x}, \mathbf{w})\), которая минимизирует данный эмпирический функционал риска. Для её поиска можно воспользоваться одним из наболее известных подходов - применении модели RankSVM, которая строится путём минимизации целевой функции \reference{Herbrich2000}
\[
\frac{1}{2} <w, w> + C \cdot \sum \limits_{i=1}^n L(f_i, \mathbf{x_i}, \mathbf{z_i}).
\] 

\par
При этом функция потерь в RankSVM является петлевой, определённой на паре \(\mathbf{x} \succ \mathbf{z}\), то есть она имеет следующий вид:
\[
L(f, \mathbf{x}, \mathbf{z}, y) = max(0, 1 - (f(\mathbf{x}, \mathbf{w}) - f(\mathbf{z}, \mathbf{w}))).
\]

\par
Если ввести переменные оптимизации \(\xi_i = max(0, 1 - (f(\mathbf{x_i}, \mathbf{w}) - f(\mathbf{z_i}, \mathbf{w}))\), то задача оптимизации может быть переписана как:
\begin{center}
\(\frac{1}{2}<w, w> + C \cdot \sum \limits_{i=1}^n \xi_i,\) при ограничениях \\
\(f(\mathbf{x_i}, w) - f(\mathbf{z_i}, w) \geq 1 - \xi_i, \\
\xi_i \geq 0, i = 1, \cdots, n.\)
\end{center}

\par
Если ранжирующая функция линейна, то исходная задача сводится к задаче квадратичного программирования.

%======================================================================================================================

\vspace*{1em}
\chapter{Основные элементы теории Демпстера-Шефера}

\par
Пусть \emph{U} - универсальное множество (фрейм различения). 
Предположим, что было получено \emph{N} наблюдений элемента множества \(u \in U\).
При этом каждое наблюдение получено в результате неточного измерения, приводящего к множеству значений \emph{A}.
Пусть \(c_i\) -- количество появлений множества \(A_i \subseteq U\), а \(\Rho_o(U)\) -- множество всех подмножеств \emph{U}.
Частотная функция \emph{m}, называемая \emph{базовой вероятностью}, определена как \reference{Dempster1967}
\[
m : \Rho_o(U) \to [0,1], 
m(\varnothing) = 1, 
\sum \limits_{A \in \Rho_o(U)} m(A) = 1.
\]

\par
В соответствии с \reference{Dempster1967}, эта функция может быть получена как \(m(A_i) = c_i / N\).

\par
Если существует функция \(h(x)\), то её верхнее математическое ожидание может быть найдено как \reference{Strat1990}:
\[
\mathbb{\overline{E}} h = \sum \limits_{i=1}^N m(A_i) \inf_{x \in A_i} h(x), \]

\par
Нижнее математическое ожидание может быть найдено аналогичным образом. 

%======================================================================================================================

\vspace*{1em}
\chapter{Формальная постановка задачи классификации в терминах неточных сравнений}

\par
Предположим, что есть \emph{M} объектов для сравнения \(\mathbf{\Psi} = \{\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_M}\}\). 
Отсюда следует, что имеется \(N = M(M - 1)\) пар для сравнения. 
Эти \emph{N} пар \((\mathbf{x}, \mathbf{z})\) составляют фрейм различения \(\Omega\). 
Предположим, что обучающая выборка состоит из \(b \leq N\) оценок вида \(\mathbf{A} \succ \mathbf{B}\), где \(\mathbf{A}\) и \(\mathbf{B}\) - некоторые множества объектов, таких что \(\mathbf{A} \cap \mathbf{B} = \varnothing\).  
Точные оценки \(\mathbf{x} \succ \mathbf{z}\) могут рассматриваться как частный случай. ``Неточные'' оценки означают отсутствие информации о том, как элементы из \(\mathbf{A}\) сравнимы между собой. 

\par
Базовые вероятности (БВ) неточных оценок могу быть определены следующим образом. 
Пусть \emph{n} -- множество пар \((i, j)\) индексов, соответствующих оценкам \(\mathbf{A_i} \succ \mathbf{B_j}\) как \(\Rho\). 
БВ оценки \(\mathbf{A_i} \succ \mathbf{B_j}\) определяется как \(m(\mathbf{A_i} \succ \mathbf{B_j}) = c_{ij} / n\), где \(c_{ij}\) - это количество оценок \(\mathbf{A_i} \succ \mathbf{B_j}\) в \(\Rho\). 
Далее для краткости предполагается, что \(c_{ij} = 1\) и \(m(\mathbf{A_i} \succ \mathbf{B_j}) = 1/n\) для всех наблюдаемых сравнений. 
Тогда можно записать нижнюю границу функционала риска как:
\[
\underline{R}(\mathbf{w}) = \mathbb{\underline{E}}L = \frac{1}{n} \sum \limits_{(i,j) \in \Rho} \underset{%
\mathbf{x} \in \mathbf{A_i}, \mathbf{z} \in \mathbf{B_j}%
}{\operatorname{min}}\]
\[
L(f(\mathbf{x}, \mathbf{w}) - f(\mathbf{z}, \mathbf{w})).
\]

\par
В свою очередь, верхняя граница может быть получена путём замены минимума на максимум в формуле, приведённой выше.

\par
Неточные оценки сравнений образуют множество распределений вероятности, где каждое распределение может быть использовано для вычисления функционала риска.
Среди них выделяем то, которое максимизирует оценку риска для каждого \(\mathbf{w}\).
В этом случае имеет место минимаксная (пессимистичная) стратегия: выбирается ``наихудшее'' распределение, дающее наибольшее значение функционала риска. 
Этот критерий выступает как страховка против наихудшего случая, так как он нацелен на минимизацию потерь в наименее благоприятном случае \reference{Robert1994}. 

%======================================================================================================================

\vspace*{1em}
\chapter{Минимаксная стратегия}

\par
Наша цель состоит в минимизации верхней оценки риска над множеством параметров \(\mathbf{w}\). 
Для этого введём новые переменные оптимизации \(G_{ij} \in \Rho\) такие, что
\[
G_{ij} = \underset{\mathbf{x} \in \mathbf{A_i}, \mathbf{z} \in \mathbf{B_j}}{\operatorname{max}} L (f (\mathbf{x}, \mathbf{w}) - f(\mathbf{z}, \mathbf{w})). 
\]

\par
Тогда задача оптимизации может быть переписана как:
\begin{center}
\(\overline{R}(\mathbf{w}) = \underset{\mathbf{w}, G_{ij}}{\operatorname{min}} \sum \limits_{(i, j) \in \Rho} G_{ij}\), при ограничениях\\
\(G_{ij} \geq L (f(\mathbf{x}, \mathbf{w}) - f(\mathbf{z}, \mathbf{w})), 
\), \(\forall \mathbf{x} \in \mathbf{A_i}\),\\
\(\forall \mathbf{z} \in \mathbf{B_j}, (i, j) \in \Rho\)
\end{center}

\par
Здесь \(\mathbf{x}\) и \(\mathbf{z}\) могут быть только из \(\Psi\). 
Далее можно снова использовать линейную функцию \emph{f} и петлевую функцию потерь, определённую на паре объектов \(\mathbf{x} \succ \mathbf{z}\):
\[
L(f, \mathbf{x}, \mathbf{z}) = max (0,1 - f(\mathbf{x}, \mathbf{w}) + f(\mathbf{z}, \mathbf{w})),
\]

В этом случае получаем:
\begin{center}
\(\overline{R}(\mathbf{w}) = \underset{\mathbf{w}, G_{ij}}{\operatorname{min}} \sum \limits_{(i, j) \in \Rho} G_{ij},\) при ограничениях\\
\(G_{ij} + \langle w, \mathbf{x} - \mathbf{z} \rangle \geq 1, \forall \mathbf{x} \in \mathbf{A_i}, \forall \mathbf{z} \in \mathbf{B_j}, \)\\
\(G_{ij} \succ 0, (i, j) \in \Rho.\)
\end{center}

\par
В результате получается задача линейного программирования. 
При этом она отличается от подобных задач наличием дополнительных ограничений для каждой переменной \(G_{ij}\). 

%======================================================================================================================

\vspace*{1em}
\chapter{Заключение}

\par
В данной статье был сформулирован подход, позволяющий проводить классификацию документов при групповых экспертных оценках. 
Определены функционалы риска с использованием минимаксной стратегии. 
Получены оптимизационные задачи, решение которых в силу своей сложности требует отдельного исследования. 

%======================================================================================================================

\begin{footnotesize}
\begin{thebibliography}{9}
	\bibitem{Herbrich2000} R. Herbrich, T. Graepel, K. Obermayer. Large margin rank boundaries for ordinal regression. Smola, Bartlett, Schoelkopf, and Schuurmans, editors, Advances in Large Margin Classifiers, pp. 115–132. MIT Press, Cambridge, MA, 2000%
	\bibitem{Wolfowitz1942} J. Wolfowitz. Additive partition functions and a class of statistical hypotheses. Annals of Mathematical Statistics, 13(3), pp. 247–279, 1942%
	\bibitem{Liu2011} T.Y. Liu. Learning to Rank for Information Retrieval, Springer, 2011%
	\bibitem{Dempster1967} A.P. Dempster. Upper and lower probabilities induced by a multi-valued mapping. Annales of Mathematical Statistics, 38(2), pp. 325–339, 1967%
	\bibitem{Strat1990} T.M. Strat. Decision analysis using belief functions. International Journal of Approximate Reasoning, 4(5), pp. 391–418, 1990%
	\bibitem{Robert1994} C.P. Robert. The Bayesian Choice. Springer, New York, 1994%
\end{thebibliography}
\end{footnotesize}

\end{multicols}
\end{document}
